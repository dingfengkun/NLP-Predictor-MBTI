{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 数据预处理类\n",
    "class MBTIDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # 文本序列化\n",
    "        sequence = self.tokenizer.texts_to_sequences([text])\n",
    "        padded_sequence = pad_sequences(sequence, maxlen=self.max_len, padding='post')[0]\n",
    "        \n",
    "        return {\n",
    "            'text': torch.tensor(padded_sequence, dtype=torch.long),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# LSTM模型架构\n",
    "class MBTILSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        \n",
    "        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(lstm_output_dim, lstm_output_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(lstm_output_dim//2, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        # LSTM层\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # 处理双向输出\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1]\n",
    "        \n",
    "        return self.fc(hidden)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数配置\n",
    "MAX_WORDS = 10000\n",
    "MAX_LEN = 500\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 16  # 16种MBTI类型\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.2\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 0.001\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#可以换一个Tokenizer\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# 数据准备\n",
    "\n",
    "df = pd.read_csv('MBTI 500.csv')  \n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.classes_ = [\n",
    "    'INTP', 'ENTP', 'INFJ', 'ENFJ', 'INTJ', 'ENTJ', 'INFP', 'ENFP',\n",
    "    'ISTJ', 'ESTJ', 'ISFJ', 'ESFJ', 'ISTP', 'ESTP', 'ISFP', 'ESFP'\n",
    "]  # MBTI 类型\n",
    "texts = df['posts'].values\n",
    "labels = label_encoder.fit_transform(df['type'].values)\n",
    "\n",
    "# 创建Tokenizer\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# 划分数据集\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 创建DataLoader\n",
    "train_dataset = MBTIDataset(X_train, y_train, tokenizer, MAX_LEN)\n",
    "val_dataset = MBTIDataset(X_val, y_val, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "#whole dataset\n",
    "X_test,y_test =  texts, labels\n",
    "test_dataset = MBTIDataset(X_test, y_test,  tokenizer, MAX_LEN)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MBTILSTM(MAX_WORDS, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, \n",
    "                N_LAYERS, BIDIRECTIONAL, DROPOUT).to(device)\n",
    "\n",
    "# 定义优化器和损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 1326/1326 [01:41<00:00, 13.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.0893 | Train Acc: 24.67%\n",
      "Val Loss: 2.0564 | Val Acc: 25.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|██████████| 1326/1326 [01:47<00:00, 12.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7098 | Train Acc: 42.34%\n",
      "Val Loss: 1.3605 | Val Acc: 54.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|██████████| 1326/1326 [01:37<00:00, 13.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9938 | Train Acc: 69.86%\n",
      "Val Loss: 0.8827 | Val Acc: 73.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|██████████| 1326/1326 [01:36<00:00, 13.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7075 | Train Acc: 78.68%\n",
      "Val Loss: 0.6550 | Val Acc: 80.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|██████████| 1326/1326 [01:36<00:00, 13.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5514 | Train Acc: 83.49%\n",
      "Val Loss: 0.6191 | Val Acc: 81.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|██████████| 1326/1326 [01:36<00:00, 13.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4455 | Train Acc: 86.79%\n",
      "Val Loss: 0.6100 | Val Acc: 82.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|██████████| 1326/1326 [01:37<00:00, 13.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3539 | Train Acc: 89.54%\n",
      "Val Loss: 0.6816 | Val Acc: 80.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|██████████| 1326/1326 [01:38<00:00, 13.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2702 | Train Acc: 92.08%\n",
      "Val Loss: 0.7127 | Val Acc: 81.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|██████████| 1326/1326 [01:36<00:00, 13.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2097 | Train Acc: 93.83%\n",
      "Val Loss: 0.7916 | Val Acc: 81.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|██████████| 1326/1326 [01:35<00:00, 13.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1653 | Train Acc: 95.28%\n",
      "Val Loss: 0.8404 | Val Acc: 80.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|██████████| 1326/1326 [01:35<00:00, 13.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1421 | Train Acc: 95.88%\n",
      "Val Loss: 0.9014 | Val Acc: 81.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|██████████| 1326/1326 [01:35<00:00, 13.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1199 | Train Acc: 96.50%\n",
      "Val Loss: 0.9344 | Val Acc: 81.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|██████████| 1326/1326 [01:35<00:00, 13.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1015 | Train Acc: 96.95%\n",
      "Val Loss: 1.1148 | Val Acc: 80.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|██████████| 1326/1326 [01:36<00:00, 13.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0893 | Train Acc: 97.35%\n",
      "Val Loss: 1.0553 | Val Acc: 79.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|██████████| 1326/1326 [01:37<00:00, 13.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0819 | Train Acc: 97.52%\n",
      "Val Loss: 1.0797 | Val Acc: 80.75%\n",
      "Final Test Accuracy: 82.06%\n"
     ]
    }
   ],
   "source": [
    "# 训练循环\n",
    "best_val_acc = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    # 训练阶段\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        texts = batch['text'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_acc += (outputs.argmax(1) == labels).sum().item()\n",
    "    \n",
    "    # 验证阶段\n",
    "    model.eval()\n",
    "    val_loss, val_acc = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            texts = batch['text'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_acc += (outputs.argmax(1) == labels).sum().item()\n",
    "    \n",
    "    # 计算指标\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc /= len(train_dataset)\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc /= len(val_dataset)\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}%\")\n",
    "\n",
    "# 测试模型\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "test_acc = 0\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        texts = batch['text'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(texts)\n",
    "        test_acc += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc /= len(val_dataset)\n",
    "print(f'Final Test Accuracy: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Final Test Accuracy: 89.59%\n"
     ]
    }
   ],
   "source": [
    "# 测试整个数据集\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "test_acc = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        texts = batch['text'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(texts)\n",
    "        test_acc += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc /= len(test_dataset)\n",
    "print(f' Final Test Accuracy: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f097e6e3a2ac415aa5d1765b1c4d8677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='输入:', layout=Layout(width='400px'), placeholder='请输入文本...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91656d91f0c04668bb65396de238cd89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='预测', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f2edda306a4a669be5f8c992f77967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 确保 `device` 设定正确\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载模型\n",
    "model.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 🔹 加载 tokenizer（确保已加载）\n",
    "# tokenizer = ...  # 需要手动提供 tokenizer\n",
    "\n",
    "# 🔹 定义 LabelEncoder 并加载 MBTI 标签\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.classes_ = np.array([\n",
    "    'INTP', 'ENTP', 'INFJ', 'ENFJ', 'INTJ', 'ENTJ', 'INFP', 'ENFP',\n",
    "    'ISTJ', 'ESTJ', 'ISFJ', 'ESFJ', 'ISTP', 'ESTP', 'ISFP', 'ESFP'\n",
    "])  # MBTI 类型\n",
    "\n",
    "# 创建输入框\n",
    "text_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='请输入文本...',\n",
    "    description='输入:',\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "# 创建预测按钮\n",
    "predict_button = widgets.Button(description=\"预测\")\n",
    "\n",
    "# 创建退出按钮\n",
    "exit_button = widgets.Button(description=\"退出\", button_style='danger')\n",
    "\n",
    "# 创建输出框\n",
    "output = widgets.Output()\n",
    "\n",
    "# 预测函数\n",
    "def predict_mbti(b):\n",
    "    with output:\n",
    "        output.clear_output()  # 清除之前的输出\n",
    "        user_input = text_input.value\n",
    "        if not user_input.strip():\n",
    "            print(\"⚠️ 请输入文本内容！\")\n",
    "            return\n",
    "        \n",
    "        # 处理输入文本\n",
    "        sequence = tokenizer.texts_to_sequences([user_input])\n",
    "        padded_sequence = pad_sequences(sequence, maxlen=MAX_LEN)\n",
    "        texts = torch.tensor(padded_sequence, dtype=torch.long).to(device)\n",
    "\n",
    "        # 进行预测\n",
    "        with torch.no_grad():\n",
    "            outputs = model(texts)\n",
    "            predicted_label = outputs.argmax(1).item()\n",
    "            predicted_mbti = label_encoder.inverse_transform(np.array([predicted_label]))[0]\n",
    "\n",
    "        # 输出结果\n",
    "        print(f\"🧠 预测的 MBTI 类型: **{predicted_mbti}**\")\n",
    "\n",
    "# 退出函数\n",
    "def exit_notebook(b):\n",
    "    print(\"📌 退出程序...\")\n",
    "    sys.exit()  # 安全退出 Python 进程\n",
    "\n",
    "# 绑定按钮点击事件\n",
    "predict_button.on_click(predict_mbti)\n",
    "exit_button.on_click(exit_notebook)  # 绑定退出功能\n",
    "\n",
    "# 显示 GUI\n",
    "display(text_input, predict_button, exit_button, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
